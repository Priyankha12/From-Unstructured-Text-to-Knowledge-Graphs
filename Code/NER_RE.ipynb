{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ADBI_cap.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"6mfFAmQ4uiW3","colab_type":"code","outputId":"bc878440-3556-40e0-b6b3-34ebca2bb052","executionInfo":{"status":"ok","timestamp":1556937895905,"user_tz":240,"elapsed":38265,"user":{"displayName":"Vidhyalakshimi Sreenivasan","photoUrl":"https://lh4.googleusercontent.com/-USDjX2HH3qk/AAAAAAAAAAI/AAAAAAAAAAc/AhuvJIOSFK0/s64/photo.jpg","userId":"08716189862880610765"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["# This cell is required for Colab Notebooks to mount our Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zf1eqIeEuzwH","colab_type":"code","outputId":"f21ace2f-9285-4dd0-c406-240238c93a1f","executionInfo":{"status":"ok","timestamp":1556937945361,"user_tz":240,"elapsed":15204,"user":{"displayName":"Vidhyalakshimi Sreenivasan","photoUrl":"https://lh4.googleusercontent.com/-USDjX2HH3qk/AAAAAAAAAAI/AAAAAAAAAAc/AhuvJIOSFK0/s64/photo.jpg","userId":"08716189862880610765"}},"colab":{"base_uri":"https://localhost:8080/","height":1112}},"source":["# Install spacy and textacy before running the subsequent cells\n","\n","# ! pip install spacy\n","! pip install textacy"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting textacy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/fb/323c81288ab9b0b9cc955dcebfd04773d7982307c1a6d559b5a30000825b/textacy-0.6.3-py2.py3-none-any.whl (145kB)\n","\u001b[K     |████████████████████████████████| 153kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.16.3)\n","Collecting mwparserfromhell>=0.4.4 (from textacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a2/5d537c16e761db25592b2e69c0071aee475f200ef231ee5ca1a499ae54bc/mwparserfromhell-0.5.3.tar.gz (132kB)\n","\u001b[K     |████████████████████████████████| 133kB 44.7MB/s \n","\u001b[?25hRequirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n","Collecting python-levenshtein>=0.12.0 (from textacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n","\u001b[K     |████████████████████████████████| 51kB 19.3MB/s \n","\u001b[?25hCollecting pyphen>=0.9.4 (from textacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 43.0MB/s \n","\u001b[?25hRequirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.3)\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.21.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.2.1)\n","Requirement already satisfied: scikit-learn>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.20.3)\n","Collecting ijson>=2.3 (from textacy)\n","  Downloading https://files.pythonhosted.org/packages/7f/e9/8508c5f4987ba238a2b169e582c1f70a47272b22a2f1fb06b9318201bb9e/ijson-2.3-py2.py3-none-any.whl\n","Requirement already satisfied: spacy>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.0.18)\n","Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.28.1)\n","Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.9.0.1)\n","Collecting srsly>=0.0.5 (from textacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/97/47753e3393aa4b18de9f942fac26f18879d1ae950243a556888f389d1398/srsly-0.0.5-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n","\u001b[K     |████████████████████████████████| 184kB 46.3MB/s \n","\u001b[?25hCollecting ftfy<5.0.0,>=4.2.0 (from textacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 19.0MB/s \n","\u001b[?25hRequirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (3.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein>=0.12.0->textacy) (41.0.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.11->textacy) (4.4.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.2)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2019.3.9)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (0.9.6)\n","Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (6.12.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (2.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (1.0.2)\n","Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (1.35)\n","Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (0.2.9)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (2.0.1)\n","Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (2018.1.10)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.9.0)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy) (1.0.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy) (0.1.7)\n","Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (1.12.0)\n","Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (0.4.3.2)\n","Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (0.5.6)\n","Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (1.10.11)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy) (0.5.1)\n","Building wheels for collected packages: mwparserfromhell, python-levenshtein, ftfy\n","  Building wheel for mwparserfromhell (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/68/e9/75/f7fcd23cc94c77fc6d07793abcabda7c8829042c177f47028b\n","  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n","Successfully built mwparserfromhell python-levenshtein ftfy\n","Installing collected packages: mwparserfromhell, python-levenshtein, pyphen, ijson, srsly, ftfy, textacy\n","Successfully installed ftfy-4.4.3 ijson-2.3 mwparserfromhell-0.5.3 pyphen-0.9.5 python-levenshtein-0.12.0 srsly-0.0.5 textacy-0.6.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9XirMQAxuaKi","colab_type":"code","colab":{}},"source":["# Import the below packages.\n","\n","import spacy\n","import textacy\n","from textacy.extract import subject_verb_object_triples\n","from bs4 import BeautifulSoup\n","import requests\n","import re\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_icbgxdruaKv","colab_type":"text"},"source":["# Loading the input data"]},{"cell_type":"code","metadata":{"id":"b5TVhXxeuaKy","colab_type":"code","colab":{}},"source":["# Execute the commented line below if running locally, if running on Google Colab use the uncommented statement \n","# data_dir stores the directory where we store the PreprocessedDataset (i.e., text files after using stanford-corenlp-python)\n","# We read the text present in every file and save it in a list called TEXTS\n","\n","# data_dir = os.getcwd()+'/ActorsPreprocessedDataset/'\n","data_dir ='gdrive/My Drive/ADBI Project/ADBI_Capstone_Project/Dataset/ActorsPreprocessedDataset/' \n","TEXTS = [open(data_dir+f).read() for f in os.listdir(data_dir)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-7k-v3KuaK6","colab_type":"text"},"source":["# Subject Verb Object Generation using Spacy's subject_verb_object_triples method"]},{"cell_type":"code","metadata":{"id":"qJoE0ZzSuaK8","colab_type":"code","colab":{}},"source":["# Download and load SpaCy's en_core_web_sm model to do the find the named entities \n","# Store the entities and its type/annotations in a dictionary for using it in KG construction\n","# Extract the SVO (Subject Verb Object) triples suing dependency parsing and save it in the svos list of tuples\n","# Also save the labels of the SVO as a separate list\n","\n","nlp = spacy.load('en_core_web_sm')\n","final_svos = []\n","final_text_svos = []\n","entity_dict = {}\n","svo_labels = []\n","for i, text in enumerate(TEXTS):\n","    doc = nlp(text)\n","    for ent in doc.ents:\n","        if ent not in entity_dict.keys():\n","            entity_dict[str(ent)] = ent.label_       \n","    svos = list(subject_verb_object_triples(doc))\n","    svos_text = [(str(x[0]).strip(), str(x[1]).strip(), str(x[2]).strip()) for x in svos]\n","    final_svos = final_svos + svos\n","    final_text_svos = final_text_svos + svos_text\n","\n","for svo in final_text_svos:\n","    tup = ['Object', 'Object']\n","    if(svo[0] in entity_dict.keys()):\n","        tup[0] = entity_dict[svo[0]]\n","    \n","    if(svo[2] in entity_dict.keys()):\n","        tup[1] = entity_dict[svo[2]]\n","    svo_labels.append(tuple(tup))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TU5rchmSuaLE","colab_type":"code","outputId":"3b8cb44f-a6a2-42ce-cb1c-7f2df863ccc8","executionInfo":{"status":"ok","timestamp":1556938590501,"user_tz":240,"elapsed":422,"user":{"displayName":"Vidhyalakshimi Sreenivasan","photoUrl":"https://lh4.googleusercontent.com/-USDjX2HH3qk/AAAAAAAAAAI/AAAAAAAAAAc/AhuvJIOSFK0/s64/photo.jpg","userId":"08716189862880610765"}},"colab":{"base_uri":"https://localhost:8080/","height":763}},"source":["final_text_svos"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Jason Shannon Acuna', 'is', 'skateboarder'),\n"," ('Jason Shannon Acuna', 'is', 'television personality'),\n"," ('Jason Shannon Acuna', 'is', 'one'),\n"," ('host', 'skateboarding', 'show'),\n"," ('Walter Abel', 'was', 'film stage'),\n"," ('Walter Abel', 'was', 'radio actor'),\n"," ('Bruce Paul Abbott', 'is', 'actor'),\n"," ('beginning', 'gained', 'notoriety'),\n"," ('Jacob Allen Abel', 'is', 'actor'),\n"," ('Jacob Allen Abel', 'is', 'singer'),\n"," ('Philip Abbott', 'was', 'character actor'),\n"," ('Willie Aames', 'is', 'actor film'),\n"," ('Willie Aames', 'is', 'director television producer'),\n"," ('Willie Aames', 'is', 'screenwriter'),\n"," ('Eight', 'Is', 'Lembeck'),\n"," ('Rodolfo Perez Acosta', 'was', 'MexicanAmerican character actor'),\n"," ('Yousef AbuTaleb', 'is', 'actor'),\n"," ('Yousef AbuTaleb', 'is', 'producer'),\n"," ('Jay Acovone', 'is', 'actor'),\n"," ('Lee William Aaker', 'is', 'child actor'),\n"," ('Jensen Ross Ackles', 'is', 'actor'),\n"," ('Jensen Ross Ackles', 'is', 'director'),\n"," ('which', 'earned', 'Ackles'),\n"," ('which', 'earned', 'Award nominations'),\n"," ('Quinton Aaron', 'is', 'actor'),\n"," ('Quinton Aaron', 'made', 'Aaron film debut'),\n"," ('Christopher Jacob Abbott', 'is', 'actor'),\n"," ('Kirk M. Acevedo', 'is', 'actor'),\n"," ('F. Murray', 'is', 'actor'),\n"," ('known', 'is', 'actor television producer entrepreneur'),\n"," ('known', 'is', 'investor'),\n"," ('Born', 'began', 'selling'),\n"," ('Victor Aaron Ramirez', 'was', 'actor'),\n"," ('Victor Aaron Ramirez', 'was', 'voice'),\n"," ('known', 'is', 'rapper guitarist'),\n"," ('known', 'is', 'actor'),\n"," ('Alexander Bud Abbott', 'was', 'actor'),\n"," ('Zachary Burr Abel', 'is', 'actor'),\n"," ('It', 'Break', 'It'),\n"," ('Jon Avery Abrahams', 'is', 'actor'),\n"," ('roles', 'are', 'Bobby'),\n"," ('Abbott', 'were', 'comedy duo'),\n"," ('Costello', 'were', 'comedy duo')]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"twFvafAGuaLU","colab_type":"code","colab":{}},"source":["# Write all the SVOs as a CSV file\n","\n","import csv\n","\n","with open('svos.csv', 'w') as csvFile:\n","    writer = csv.writer(csvFile)\n","    writer.writerows(final_text_svos)\n","\n","csvFile.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vZNyEgtuaLb","colab_type":"code","outputId":"7195d87f-832e-4671-cea5-df6e753b4eef","executionInfo":{"status":"ok","timestamp":1556938594332,"user_tz":240,"elapsed":331,"user":{"displayName":"Vidhyalakshimi Sreenivasan","photoUrl":"https://lh4.googleusercontent.com/-USDjX2HH3qk/AAAAAAAAAAI/AAAAAAAAAAc/AhuvJIOSFK0/s64/photo.jpg","userId":"08716189862880610765"}},"colab":{"base_uri":"https://localhost:8080/","height":763}},"source":["svo_labels"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Object', 'Object'),\n"," ('Object', 'Object'),\n"," ('Object', 'CARDINAL'),\n"," ('Object', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('Object', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('CARDINAL', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('Object', 'Object'),\n"," ('Object', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('Object', 'Object'),\n"," ('Object', 'Object'),\n"," ('Object', 'Object'),\n"," ('Object', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('Object', 'Object'),\n"," ('Object', 'Object'),\n"," ('Object', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('Object', 'Object'),\n"," ('PERSON', 'Object'),\n"," ('Object', 'PERSON'),\n"," ('NORP', 'Object'),\n"," ('PERSON', 'Object')]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"0F-L5yoQuaLo","colab_type":"code","colab":{}},"source":["# Save the entity type dictionary using pickle\n","\n","import pickle\n","with open('entity_dict.pickle', 'wb') as handle:\n","    pickle.dump(entity_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a44aYM--xhGN","colab_type":"text"},"source":["##Experimental section:\n","Other techniques we tried out. "]},{"cell_type":"markdown","metadata":{"id":"a5ghRP6NuaLx","colab_type":"text"},"source":["### Knowledge Graph visualization"]},{"cell_type":"code","metadata":{"id":"IVngZg8WuaL0","colab_type":"code","colab":{}},"source":["# Visualize the KG using Graphviz\n","\n","def generate_graphviz_graph(entity_relations, name, verbose=True):\n","    \"\"\"digraph G {\n","    # a -> b [ label=\"a to b\" ];\n","    # b -> c [ label=\"another label\"];\n","    }\"\"\"\n","    graph = list()\n","    graph.append('digraph {')\n","    for er in entity_relations:\n","        graph.append('\"{}\" -> \"{}\" [ label=\"{}\" ];'.format(er[0], er[2], er[1]))\n","    graph.append('}')\n","\n","    out_dot = name + '.dot'\n","    with open(out_dot, 'w') as output_file:\n","        output_file.writelines(graph)\n","\n","    out_png = name + '.png'\n","    DOT_BIN_PATH = 'dot'\n","    command = \"dot -Tpng {} -o {}\".format(out_dot, out_png)\n"," \n","    os.system(command)\n","\n","    print('Wrote graph to {} and {}'.format(out_dot, out_png))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w9zAMV_2uaMC","colab_type":"text"},"source":["### Subject Verb Object using Spacy's Github code"]},{"cell_type":"code","metadata":{"id":"azYVhVK1uaMJ","colab_type":"code","colab":{}},"source":["# Define the dependencies that are applicable to subjects and Objects\n","\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import spacy\n","from spacy.lang.en import English\n","\n","SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n","OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-dRkLgeuaMU","colab_type":"code","colab":{}},"source":["#Every function has a set of rules to extract Subjects and Objects based on the rules \n","\n","def getSubsFromConjunctions(subs):\n","    moreSubs = []\n","    for sub in subs:\n","        # rights is a generator\n","        rights = list(sub.rights)\n","        rightDeps = {tok.lower_ for tok in rights}\n","        if \"and\" in rightDeps:\n","            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n","            if len(moreSubs) > 0:\n","                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n","    return moreSubs\n","\n","def getObjsFromConjunctions(objs):\n","    moreObjs = []\n","    for obj in objs:\n","        # rights is a generator\n","        rights = list(obj.rights)\n","        rightDeps = {tok.lower_ for tok in rights}\n","        if \"and\" in rightDeps:\n","            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n","            if len(moreObjs) > 0:\n","                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n","    return moreObjs\n","\n","def getVerbsFromConjunctions(verbs):\n","    moreVerbs = []\n","    for verb in verbs:\n","        rightDeps = {tok.lower_ for tok in verb.rights}\n","        if \"and\" in rightDeps:\n","            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n","            if len(moreVerbs) > 0:\n","                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n","    return moreVerbs\n","\n","def findSubs(tok):\n","    head = tok.head\n","    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n","        head = head.head\n","    if head.pos_ == \"VERB\":\n","        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n","        if len(subs) > 0:\n","            verbNegated = isNegated(head)\n","            subs.extend(getSubsFromConjunctions(subs))\n","            return subs, verbNegated\n","        elif head.head != head:\n","            return findSubs(head)\n","    elif head.pos_ == \"NOUN\":\n","        return [head], isNegated(tok)\n","    return [], False\n","\n","def isNegated(tok):\n","    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n","    for dep in list(tok.lefts) + list(tok.rights):\n","        if dep.lower_ in negations:\n","            return True\n","    return False\n","\n","def findSVs(tokens):\n","    svs = []\n","    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n","    for v in verbs:\n","        subs, verbNegated = getAllSubs(v)\n","        if len(subs) > 0:\n","            for sub in subs:\n","                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n","    return svs\n","\n","def getObjsFromPrepositions(deps):\n","    objs = []\n","    for dep in deps:\n","        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n","            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n","    return objs\n","\n","def getObjsFromAttrs(deps):\n","    for dep in deps:\n","        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n","            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n","            if len(verbs) > 0:\n","                for v in verbs:\n","                    rights = list(v.rights)\n","                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n","                    objs.extend(getObjsFromPrepositions(rights))\n","                    if len(objs) > 0:\n","                        return v, objs\n","    return None, None\n","\n","def getObjFromXComp(deps):\n","    for dep in deps:\n","        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n","            v = dep\n","            rights = list(v.rights)\n","            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n","            objs.extend(getObjsFromPrepositions(rights))\n","            if len(objs) > 0:\n","                return v, objs\n","    return None, None\n","\n","def getAllSubs(v):\n","    verbNegated = isNegated(v)\n","    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n","    if len(subs) > 0:\n","        subs.extend(getSubsFromConjunctions(subs))\n","    else:\n","        foundSubs, verbNegated = findSubs(v)\n","        subs.extend(foundSubs)\n","    return subs, verbNegated\n","\n","def getAllObjs(v):\n","    # rights is a generator\n","    rights = list(v.rights)\n","    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n","    objs.extend(getObjsFromPrepositions(rights)\n","    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n","    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n","        objs.extend(potentialNewObjs)\n","        v = potentialNewVerb\n","    if len(objs) > 0:\n","        objs.extend(getObjsFromConjunctions(objs))\n","    return v, objs\n","\n","def findSVOs(tokens):\n","    svos = []\n","    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n","    for v in verbs:\n","        subs, verbNegated = getAllSubs(v)\n","        # hopefully there are subs, if not, don't examine this verb any longer\n","        if len(subs) > 0:\n","            v, objs = getAllObjs(v)\n","            for sub in subs:\n","                for obj in objs:\n","                    objNegated = isNegated(obj)\n","                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n","    return svos\n","def printDeps(toks):\n","    for tok in toks:\n","        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mP42XtOIuaMa","colab_type":"code","outputId":"93ce625a-18de-4b2e-c80d-ec23aa1bcd9c","colab":{}},"source":["# Example SVOs\n","\n","svos = findSVOs(doc)\n","print(svos)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('vinci', 'was', 'polymath'), ('areas', 'included', 'invention'), ('he', 'called', 'father'), ('he', 'considered', 'one'), ('he', 'epitomised', 'ideal')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S0Ta9MhsuaMm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}